{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IV-TfusQClaZ"
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from collections import OrderedDict\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles, links, titles = [], [], []\n",
    "\n",
    "with open('articles.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    for row in csv_reader:\n",
    "        articles.append(row[5])\n",
    "        links.append(row[3])\n",
    "        titles.append(row[4])\n",
    "\n",
    "t, av = [], 0\n",
    "with open('processed_articles.csv', 'r') as read_obj:\n",
    "    csv_reader = csv.reader(read_obj)\n",
    "    for row in csv_reader:\n",
    "        t.append(row)\n",
    "        av+= len(row)\n",
    "        \n",
    "av /= len(t)     \n",
    "\n",
    "# import csv\n",
    "\n",
    "# with open('titles.csv','w') as result_file:\n",
    "#     wr = csv.writer(result_file, dialect='excel')\n",
    "#     wr.writerow(titles)\n",
    "    \n",
    "    \n",
    "# with open('links.csv','w') as result_file:\n",
    "#     wr = csv.writer(result_file, dialect='excel')\n",
    "#     wr.writerow(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(article) for article in articles]\n",
    "tokens = [[word for word in article if word.isalpha()] for article in tokens] \n",
    "ps = PorterStemmer()\n",
    "\n",
    "stem_tokens = [[ps.stem(word) for word in article] for article in tokens]\n",
    "tokens = [[word for word in article if not word in stopwords.words(\"english\")] for article in stem_tokens]\n",
    "\n",
    "\n",
    "# with open('processed_articles.csv','w') as result_file:\n",
    "#     wr = csv.writer(result_file, dialect='excel')\n",
    "#     wr.writerow(tokens)\n",
    "    \n",
    "with open(\"processed_articles.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHLqfP9tvt59"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ob4RnAtxrWH_",
    "outputId": "c63a69ac-cfda-4ce4-b93a-05ea1c3daf60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the index terms - 6089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['machin', 'learn', 'involv', 'neural', 'network']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [word_tokenize(article) for article in articles]\n",
    "tokens = [[word for word in article if word.isalpha()] for article in tokens] \n",
    "ps = PorterStemmer()\n",
    "\n",
    "stem_tokens = [[ps.stem(word) for word in article] for article in tokens]\n",
    "tokens = [[word for word in article if not word in stopwords.words(\"english\")] for article in stem_tokens]\n",
    "\n",
    "print(f'Size of the index terms - {len(set([token for article in tokens for token in article]))}')\n",
    "\n",
    "\n",
    "avg_doc_len = 0\n",
    "for doc in tokens:\n",
    "    avg_doc_len += len(doc)\n",
    "\n",
    "avg_doc_len /= 100\n",
    "\n",
    "sample_query = 'Machine Learning can involve neural networks'\n",
    "\n",
    "def preprocess_query(query):\n",
    "    query_terms = query.split(\" \")  #Split the query -> tokenisation\n",
    "\n",
    "    #Query normalisation\n",
    "    query_terms = [term for term in query_terms if term.isalpha()]\n",
    "    query_terms = [ps.stem(term) for term in query_terms]\n",
    "    query_terms = [word for word in query_terms if not word in stopwords.words(\"english\")]\n",
    "\n",
    "    return query_terms\n",
    "\n",
    "preprocess_query(sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "\n",
    "#['', 'article_id', 'headline', 'desc', 'date', 'url', 'articles', 'article_type', 'article_length']\n",
    "\n",
    "articles, links, titles = [], [], []\n",
    "with open('data_ie/indian_express_dataset.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    next(csv_reader)\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "#         for thing in row:\n",
    "#             print(thing)\n",
    "        articles.append(row[6])\n",
    "        links.append(row[5])\n",
    "        titles.append(row[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 2500\n",
    "\n",
    "short_dataset = []\n",
    "for a, l, t in zip(articles[:cutoff], links[:cutoff], titles[:cutoff]):\n",
    "    join = [t, l, a]\n",
    "    short_dataset.append(join)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_ie/indian_express_dataset_short.csv','w', newline='') as result_file:\n",
    "    wr = csv.writer(result_file, dialect='excel')\n",
    "    for item in short_dataset:\n",
    "        wr.writerow(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "with open('data_ie/indian_express_dataset_short.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        articles.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word_tokenize(article) for article in articles]\n",
    "tokens = [[word for word in article if word.isalpha()] for article in tokens] \n",
    "ps = PorterStemmer()\n",
    "\n",
    "stem_tokens = [[ps.stem(word) for word in article] for article in tokens]\n",
    "tokens = [[word for word in article if not word in stopwords.words(\"english\")] for article in stem_tokens]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_ie/processed_articles.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'aircraft', 'crash', 'odisha', 'dhenkan', 'district', 'monday', 'kill', 'traine', 'pilot', 'instructor', 'offici', 'said', 'trainer', 'aircraft', 'crash', 'tarmac', 'govern', 'aviat', 'train', 'institut', 'gati', 'birasala', 'district', 'addit', 'district', 'magistr', 'adm', 'dhenkan', 'B', 'K', 'nayak', 'said', 'two', 'taken', 'nearbi', 'hospit', 'kamakhyanagar', 'doctor', 'declar', 'dead', 'nayak', 'said', 'senior', 'polic', 'district', 'offici', 'spot', 'probe', 'accid', 'would', 'conduct', 'accid', 'might', 'occur', 'due', 'technic', 'glitch', 'offici', 'said', 'kamakhyanagar', 'polic', 'station', 'A', 'dalua', 'said', 'trainer', 'wa', 'man', 'ident', 'deceas', 'ascertain']\n"
     ]
    }
   ],
   "source": [
    "with open('data_ie/processed_articles.csv', 'r') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    for idx, row in enumerate(csv_reader):\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IR Lab 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
